{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "p1Gj-BnmbR4l"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "✅ Full training pipeline for ARIMA + Stacking model\n",
        "✅ Preprocessing pipeline using ColumnTransformer for reproducibility\n",
        "✅ Saving ARIMA model, Stacking pipeline, and Hybrid weights\n",
        "✅ Example inference script to load and predict"
      ],
      "metadata": {
        "id": "emfpFI-Gme3o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKMZHA9_lxMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab529d0-c05d-4672-90c3-886ddb1fd50a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000819 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 514\n",
            "[LightGBM] [Info] Number of data points in the train set: 11513, number of used features: 5\n",
            "[LightGBM] [Info] Start training from score 13.333459\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000165 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 511\n",
            "[LightGBM] [Info] Number of data points in the train set: 9210, number of used features: 5\n",
            "[LightGBM] [Info] Start training from score 13.334137\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000163 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 509\n",
            "[LightGBM] [Info] Number of data points in the train set: 9210, number of used features: 5\n",
            "[LightGBM] [Info] Start training from score 13.333806\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000162 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 510\n",
            "[LightGBM] [Info] Number of data points in the train set: 9210, number of used features: 5\n",
            "[LightGBM] [Info] Start training from score 13.331412\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001085 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 511\n",
            "[LightGBM] [Info] Number of data points in the train set: 9211, number of used features: 5\n",
            "[LightGBM] [Info] Start training from score 13.336253\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001012 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 511\n",
            "[LightGBM] [Info] Number of data points in the train set: 9211, number of used features: 5\n",
            "[LightGBM] [Info] Start training from score 13.331686\n",
            "ARIMA RMSE: 12485.56, Stacking RMSE: 84614.70\n",
            "Weights -> ARIMA: 0.871, Stacking: 0.129\n",
            "✅ Models and weights saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# KL High-rise Property Price Forecast with Calibrated Dynamic Features\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 1: Load MHPI_Annual.csv for ARIMA\n",
        "# -----------------------------\n",
        "mhpi_df = pd.read_csv(\"https://raw.githubusercontent.com/englian1123/KL-High-Rise-Data/refs/heads/main/MHPI_Annual.csv\")\n",
        "price_series = mhpi_df['Price']\n",
        "\n",
        "# Fit ARIMA and get in-sample predictions\n",
        "arima_model = ARIMA(price_series, order=(1, 1, 1)).fit()\n",
        "arima_in_sample = arima_model.predict(start=1, end=len(price_series)-1)\n",
        "arima_rmse = np.sqrt(mean_squared_error(price_series[1:], arima_in_sample))\n",
        "\n",
        "# Forecast 2025–2030\n",
        "arima_forecast = arima_model.forecast(steps=6)\n",
        "forecast_years = list(range(2025, 2031))\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 2: Load KLHighRise.csv for Stacking Model\n",
        "# -----------------------------\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/englian1123/KL-High-Rise-Data/refs/heads/main/KLHighRise.csv\")\n",
        "\n",
        "# Basic preprocessing\n",
        "df['ParcelArea'] = df['ParcelArea'].astype(str).str.extract(r'(\\d+\\.?\\d*)')[0].astype(np.float32)\n",
        "df['Tenure'] = df['Tenure'].map({'Freehold': 1, 'Leasehold': 0}).fillna(0).astype(np.float32)\n",
        "\n",
        "price_cap = df['TransactionPrice'].quantile(0.90)\n",
        "df['TransactionPrice'] = np.clip(df['TransactionPrice'], 0, price_cap).astype(np.float32)\n",
        "area_cap = df['ParcelArea'].quantile(0.90)\n",
        "df['ParcelArea'] = np.clip(df['ParcelArea'], 0, area_cap).astype(np.float32)\n",
        "\n",
        "df['TransactionPrice'] = np.log1p(df['TransactionPrice']).astype(np.float32)\n",
        "df['ParcelArea'] = np.log1p(df['ParcelArea']).astype(np.float32)\n",
        "\n",
        "scheme_encoding = df.groupby('SchemeName')['TransactionPrice'].mean().astype(np.float32)\n",
        "df['Scheme_Name_encoded'] = df['SchemeName'].map(scheme_encoding).fillna(scheme_encoding.mean()).astype(np.float32)\n",
        "\n",
        "df = pd.get_dummies(df, columns=['Mukim'], drop_first=True, dtype=np.float32)\n",
        "\n",
        "unit_level_map = {'03A': 4, '12B': 12, '13A': 14, '23A': 24, '33A': 34, '43A': 44, '53A': 54,\n",
        "                  'B': 0, 'D': 0, 'G': 0, 'LG': 0, 'MZ': 0, 'P': 0, 'UG': 0}\n",
        "df['UnitLevel_clean'] = df['UnitLevel'].replace(unit_level_map)\n",
        "unit_level_mean = pd.to_numeric(df['UnitLevel_clean'], errors='coerce').mean()\n",
        "df['UnitLevel_clean'] = pd.to_numeric(df['UnitLevel_clean'], errors='coerce').fillna(unit_level_mean).astype(np.float32)\n",
        "\n",
        "selected_features = ['Scheme_Name_encoded', 'ParcelArea', 'Mukim_Mukim Batu', 'UnitLevel_clean', 'Tenure']\n",
        "X = df[selected_features]\n",
        "y = df['TransactionPrice']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 3: Build Stacking Model\n",
        "# -----------------------------\n",
        "base_models = [\n",
        "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
        "    ('xgb', XGBRegressor(n_estimators=100, random_state=42, verbosity=0)),\n",
        "    ('lgbm', LGBMRegressor(n_estimators=100, random_state=42))\n",
        "]\n",
        "meta_model = LinearRegression()\n",
        "\n",
        "stacking_model = StackingRegressor(estimators=base_models, final_estimator=meta_model, passthrough=True)\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred_stack = stacking_model.predict(X_test)\n",
        "y_pred_stack_orig = np.expm1(y_pred_stack)\n",
        "y_test_orig = np.expm1(y_test)\n",
        "stacking_rmse = np.sqrt(mean_squared_error(y_test_orig, y_pred_stack_orig))\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 4: Compute Dynamic Weights\n",
        "# -----------------------------\n",
        "inv_arima = 1 / arima_rmse\n",
        "inv_stack = 1 / stacking_rmse\n",
        "weight_arima = inv_arima / (inv_arima + inv_stack)\n",
        "weight_stack = 1 - weight_arima\n",
        "\n",
        "print(f\"ARIMA RMSE: {arima_rmse:.2f}, Stacking RMSE: {stacking_rmse:.2f}\")\n",
        "print(f\"Weights -> ARIMA: {weight_arima:.3f}, Stacking: {weight_stack:.3f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 5: Save Models and Pipeline\n",
        "# -----------------------------\n",
        "# Save ARIMA model\n",
        "with open(\"arima_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(arima_model, f)\n",
        "\n",
        "# Save stacking model\n",
        "joblib.dump(stacking_model, \"stacking_model.pkl\")\n",
        "\n",
        "# Save weights\n",
        "weights = {\"weight_arima\": weight_arima, \"weight_stack\": weight_stack}\n",
        "with open(\"hybrid_weights.pkl\", \"wb\") as f:\n",
        "    pickle.dump(weights, f)\n",
        "\n",
        "print(\"✅ Models and weights saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Script"
      ],
      "metadata": {
        "id": "p1Gj-BnmbR4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load ARIMA model\n",
        "with open(\"arima_model.pkl\", \"rb\") as f:\n",
        "    arima_loaded = pickle.load(f)\n",
        "\n",
        "# Load stacking model\n",
        "stacking_loaded = joblib.load(\"stacking_model.pkl\")\n",
        "\n",
        "# Load weights\n",
        "with open(\"hybrid_weights.pkl\", \"rb\") as f:\n",
        "    weights_loaded = pickle.load(f)\n",
        "\n",
        "# Example new data for stacking model\n",
        "new_data = pd.DataFrame({\n",
        "    'Scheme_Name_encoded': [5.2],\n",
        "    'ParcelArea': [2.8],\n",
        "    'Mukim_Mukim Batu': [0],\n",
        "    'UnitLevel_clean': [12],\n",
        "    'Tenure': [1]\n",
        "})\n",
        "\n",
        "# Predict stacking price (original scale)\n",
        "stack_pred = np.expm1(stacking_loaded.predict(new_data))\n",
        "\n",
        "# Predict ARIMA next step\n",
        "arima_pred = arima_loaded.forecast(steps=1).iloc[0]\n",
        "\n",
        "# Hybrid forecast\n",
        "hybrid_pred = (weights_loaded['weight_arima'] * arima_pred) + (weights_loaded['weight_stack'] * stack_pred[0])\n",
        "print(\"Hybrid Forecast:\", hybrid_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcY7xGifoc1u",
        "outputId": "92aadb32-5310-429d-d725-ff8e07f9e11b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid Forecast: 513033.82617268054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Training + Save Pipeline\n",
        "✅ Full training pipeline for ARIMA + Stacking model\n",
        "\n",
        "✅ Preprocessing pipeline using ColumnTransformer for raw input transformation\n",
        "\n",
        "✅ Saving ARIMA model, Stacking pipeline, and Hybrid weights\n",
        "\n",
        "✅ Example inference script for **raw user input**"
      ],
      "metadata": {
        "id": "R7gi6xAKbaEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 1: Load MHPI_Annual.csv for ARIMA\n",
        "# -----------------------------\n",
        "mhpi_df = pd.read_csv(\"https://raw.githubusercontent.com/englian1123/KL-High-Rise-Data/refs/heads/main/MHPI_Annual.csv\")\n",
        "price_series = mhpi_df['Price']\n",
        "\n",
        "# Fit ARIMA and compute RMSE\n",
        "arima_model = ARIMA(price_series, order=(1, 1, 1)).fit()\n",
        "arima_in_sample = arima_model.predict(start=1, end=len(price_series)-1)\n",
        "arima_rmse = np.sqrt(mean_squared_error(price_series[1:], arima_in_sample))\n",
        "\n",
        "# Forecast 2025–2030\n",
        "arima_forecast = arima_model.forecast(steps=6)\n",
        "forecast_years = list(range(2025, 2031))\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 2: Load KLHighRise.csv for Stacking Model\n",
        "# -----------------------------\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/englian1123/KL-High-Rise-Data/refs/heads/main/KLHighRise.csv\")\n",
        "\n",
        "# Compute target encoding for SchemeName\n",
        "scheme_encoding = df.groupby('SchemeName')['TransactionPrice'].mean().astype(np.float32)\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 3: Define Custom Transformers\n",
        "# -----------------------------\n",
        "unit_level_map = {'03A': 4, '12B': 12, '13A': 14, '23A': 24, '33A': 34, '43A': 44, '53A': 54,\n",
        "                  'B': 0, 'D': 0, 'G': 0, 'LG': 0, 'MZ': 0, 'P': 0, 'UG': 0}\n",
        "\n",
        "def clean_unit_level(x):\n",
        "    # Apply replace and then infer objects to handle the FutureWarning explicitly\n",
        "    cleaned_df = pd.DataFrame(x).replace(unit_level_map).infer_objects(copy=False)\n",
        "    return cleaned_df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "\n",
        "def encode_scheme(x):\n",
        "    # x is expected to be a DataFrame with the 'SchemeName' column\n",
        "    encoded_series = x['SchemeName'].map(scheme_encoding).fillna(scheme_encoding.mean())\n",
        "    return encoded_series.to_frame(name='Scheme_Name_encoded') # Ensure 2D output\n",
        "\n",
        "def log_transform(x):\n",
        "    return np.log1p(x)\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 4: Outlier Caps\n",
        "# -----------------------------\n",
        "price_cap = df['TransactionPrice'].quantile(0.90)\n",
        "area_cap = df['ParcelArea'].astype(str).str.extract(r'(\\d+\\.?\\d*)')[0].astype(float).quantile(0.90)\n",
        "\n",
        "# Define a named function to cap outliers that captures area_cap for pickling compatibility\n",
        "def cap_parcel_area(x):\n",
        "    return np.clip(x, 0, area_cap)\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 5: Build Preprocessing Pipeline\n",
        "# -----------------------------\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('scheme', FunctionTransformer(encode_scheme), ['SchemeName']),\n",
        "    ('parcel', Pipeline(steps=[\n",
        "        ('cap', FunctionTransformer(cap_parcel_area)), # Use the named function here instead of lambda\n",
        "        ('log', FunctionTransformer(log_transform))\n",
        "    ]), ['ParcelArea']),\n",
        "    ('unit_clean', FunctionTransformer(clean_unit_level), ['UnitLevel']),\n",
        "    ('tenure', 'passthrough', ['Tenure']),\n",
        "    ('mukim', OneHotEncoder(drop='first', handle_unknown='ignore'), ['Mukim'])\n",
        "])\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 6: Prepare Data\n",
        "# -----------------------------\n",
        "df['Tenure'] = df['Tenure'].map({'Freehold': 1, 'Leasehold': 0}).fillna(0)\n",
        "df['Mukim'] = df['Mukim'].fillna('Mukim Batu')\n",
        "X = df[['SchemeName', 'ParcelArea', 'Mukim', 'UnitLevel', 'Tenure']]\n",
        "y = np.log1p(np.clip(df['TransactionPrice'], 0, price_cap))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 7: Build Stacking Model Pipeline\n",
        "# -----------------------------\n",
        "base_models = [\n",
        "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
        "    ('xgb', XGBRegressor(n_estimators=100, random_state=42, verbosity=0)),\n",
        "    ('lgbm', LGBMRegressor(n_estimators=100, random_state=42))\n",
        "]\n",
        "meta_model = LinearRegression()\n",
        "\n",
        "stacking_model = StackingRegressor(estimators=base_models, final_estimator=meta_model, passthrough=True)\n",
        "\n",
        "full_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', stacking_model)\n",
        "])\n",
        "\n",
        "full_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and RMSE\n",
        "y_pred_stack = full_pipeline.predict(X_test)\n",
        "stacking_rmse = np.sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_pred_stack)))\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 8: Compute Dynamic Weights\n",
        "# -----------------------------\n",
        "inv_arima = 1 / arima_rmse\n",
        "inv_stack = 1 / stacking_rmse\n",
        "weight_arima = inv_arima / (inv_arima + inv_stack)\n",
        "weight_stack = 1 - weight_arima\n",
        "\n",
        "print(f\"ARIMA RMSE: {arima_rmse:.2f}, Stacking RMSE: {stacking_rmse:.2f}\")\n",
        "print(f\"Weights -> ARIMA: {weight_arima:.3f}, Stacking: {weight_stack:.3f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# STEP 9: Save Models and Pipeline\n",
        "# -----------------------------\n",
        "with open(\"arima_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(arima_model, f)\n",
        "\n",
        "joblib.dump(full_pipeline, \"stacking_pipeline.pkl\")\n",
        "\n",
        "weights = {\"weight_arima\": weight_arima, \"weight_stack\": weight_stack}\n",
        "with open(\"hybrid_weights.pkl\", \"wb\") as f:\n",
        "    pickle.dump(weights, f)\n",
        "\n",
        "print(\"✅ Models, pipeline, and weights saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juLmeJKgdVBo",
        "outputId": "065e7e45-e1ca-488f-8d5b-6e82e290dbb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003228 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 528\n",
            "[LightGBM] [Info] Number of data points in the train set: 11513, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 13.333459\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000639 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 523\n",
            "[LightGBM] [Info] Number of data points in the train set: 9210, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 13.334137\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 524\n",
            "[LightGBM] [Info] Number of data points in the train set: 9210, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 13.333806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000152 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 524\n",
            "[LightGBM] [Info] Number of data points in the train set: 9210, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 13.331412\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000563 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 525\n",
            "[LightGBM] [Info] Number of data points in the train set: 9211, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 13.336253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000152 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 527\n",
            "[LightGBM] [Info] Number of data points in the train set: 9211, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 13.331686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARIMA RMSE: 12485.56, Stacking RMSE: 82479.44\n",
            "Weights -> ARIMA: 0.869, Stacking: 0.131\n",
            "✅ Models, pipeline, and weights saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Script - Raw Input"
      ],
      "metadata": {
        "id": "OmmgvVe4cFRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------\n",
        "# Load Saved Assets\n",
        "# -----------------------------\n",
        "with open(\"arima_model.pkl\", \"rb\") as f:\n",
        "    arima_loaded = pickle.load(f)\n",
        "\n",
        "stacking_pipeline_loaded = joblib.load(\"stacking_pipeline.pkl\")\n",
        "\n",
        "with open(\"hybrid_weights.pkl\", \"rb\") as f:\n",
        "    weights_loaded = pickle.load(f)\n",
        "\n",
        "# -----------------------------\n",
        "# Example Raw User Input\n",
        "# -----------------------------\n",
        "raw_input = pd.DataFrame({\n",
        "    'SchemeName': ['FERNLEA COURT'],   # Raw scheme name\n",
        "    'ParcelArea': [1200],           # Raw parcel area in sq ft\n",
        "    'Mukim': ['Mukim Batu'],        # Raw Mukim name\n",
        "    'UnitLevel': ['12B'],           # Raw unit level\n",
        "    'Tenure': [1]                   # Freehold (1), Leasehold (0)\n",
        "})\n",
        "\n",
        "# -----------------------------\n",
        "# Predict Using Stacking Pipeline\n",
        "# -----------------------------\n",
        "stack_pred = np.expm1(stacking_pipeline_loaded.predict(raw_input))  # Convert back to original scale\n",
        "\n",
        "# -----------------------------\n",
        "# Predict Using ARIMA\n",
        "# -----------------------------\n",
        "arima_pred = arima_loaded.forecast(steps=1).iloc[0]\n",
        "\n",
        "# -----------------------------\n",
        "# Compute Hybrid Forecast\n",
        "# -----------------------------\n",
        "hybrid_pred = (weights_loaded['weight_arima'] * arima_pred) + (weights_loaded['weight_stack'] * stack_pred[0])\n",
        "\n",
        "print(\"Stacking Prediction:\", stack_pred[0])\n",
        "print(\"ARIMA Prediction:\", arima_pred)\n",
        "print(\"Hybrid Forecast:\", hybrid_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUMTrmGZcG5O",
        "outputId": "1a2981f1-87d6-4647-c860-d98b92916216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacking Prediction: 1474134.6830461181\n",
            "ARIMA Prediction: 583491.828471573\n",
            "Hybrid Forecast: 700589.412364786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-701715242.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  cleaned_df = pd.DataFrame(x).replace(unit_level_map).infer_objects(copy=False)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oIdrdGA4M1yF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P2f1ntCjQKG",
        "outputId": "bf54ac8a-cdd9-4675-824e-a90f676aa378"
      },
      "source": [
        "import pickle\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load saved assets\n",
        "with open(\"arima_model.pkl\", \"rb\") as f:\n",
        "    arima_loaded = pickle.load(f)\n",
        "\n",
        "stacking_pipeline_loaded = joblib.load(\"stacking_pipeline.pkl\")\n",
        "\n",
        "with open(\"hybrid_weights.pkl\", \"rb\") as f:\n",
        "    weights_loaded = pickle.load(f)\n",
        "\n",
        "# Load historical data to get last year\n",
        "mhpi_df = pd.read_csv(\"https://raw.githubusercontent.com/englian1123/KL-High-Rise-Data/refs/heads/main/MHPI_Annual.csv\")\n",
        "last_year = int(mhpi_df['Year of Year'].iloc[-1])\n",
        "\n",
        "# --- User Input ---\n",
        "desired_year = 2028  # Change this as needed\n",
        "\n",
        "# Validate desired_year\n",
        "if not isinstance(desired_year, int):\n",
        "    raise ValueError(\"Desired year must be an integer.\")\n",
        "\n",
        "steps_ahead = desired_year - last_year\n",
        "\n",
        "# Handle cases where desired_year is not in the future\n",
        "if steps_ahead <= 0:\n",
        "    print(f\"Desired year {desired_year} is not after the last historical year {last_year}.\")\n",
        "    if desired_year == last_year:\n",
        "        print(f\"Returning the last observed price for {last_year} as the 'forecast'.\")\n",
        "        arima_pred = mhpi_df['Price'].iloc[-1]  # Use last actual price if desired_year is current year\n",
        "    else:  # desired_year < last_year\n",
        "        raise ValueError(f\"Cannot forecast for a past year ({desired_year}). Last historical year is {last_year}.\")\n",
        "else:\n",
        "    # ARIMA forecast for desired year\n",
        "    forecast_values = arima_loaded.forecast(steps=steps_ahead)\n",
        "    arima_pred = forecast_values.iloc[-1] if hasattr(forecast_values, 'iloc') else forecast_values[-1]\n",
        "\n",
        "# Example raw input for stacking model\n",
        "raw_input = pd.DataFrame({\n",
        "    'SchemeName': ['FABER INDAH'],\n",
        "    'ParcelArea': [1200],\n",
        "    'Mukim': ['Mukim Batu'],\n",
        "    'UnitLevel': ['12B'],\n",
        "    'Tenure': [1]\n",
        "})\n",
        "\n",
        "# Predict stacking price (original scale)\n",
        "stack_pred = np.expm1(stacking_pipeline_loaded.predict(raw_input))\n",
        "\n",
        "# Hybrid forecast\n",
        "hybrid_pred = (weights_loaded['weight_arima'] * arima_pred) + (weights_loaded['weight_stack'] * stack_pred[0])\n",
        "\n",
        "# Output\n",
        "print(f\"Forecast for {desired_year}:\")\n",
        "print(\"ARIMA Prediction:\", arima_pred)\n",
        "print(\"Stacking Prediction:\", stack_pred[0])\n",
        "print(\"Hybrid Forecast:\", hybrid_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forecast for 2028:\n",
            "ARIMA Prediction: 609707.0444733249\n",
            "Stacking Prediction: 559188.2407966102\n",
            "Hybrid Forecast: 603065.0669159518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-701715242.py:48: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  cleaned_df = pd.DataFrame(x).replace(unit_level_map).infer_objects(copy=False)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn nest_asyncio pyngrok pandas numpy joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qXXKqkQ9O8NS",
        "outputId": "7659ab67-d771-467b-e3d2-06206e678c51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.121.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.38.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.4.1-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: starlette<0.50.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.49.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.11.10)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.50.0,>=0.40.0->fastapi) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading pyngrok-7.4.1-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run FastAPI on Google Colab"
      ],
      "metadata": {
        "id": "dhPHSbQGPlgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn nest_asyncio pyngrok pandas numpy joblib\n",
        "\n",
        "import pickle\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import threading # Import threading module\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ngrok authentication token\n",
        "ngrok.set_auth_token(\"35T7iSFl2TobTiUGSAHkKsDqFgs_6UTj4KWQ9zfaYwTAin5GZ\")\n",
        "\n",
        "# Apply nest_asyncio for Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI(title=\"KL High-Rise Price Forecast API\")\n",
        "\n",
        "# Load saved assets\n",
        "with open(\"arima_model.pkl\", \"rb\") as f:\n",
        "    arima_loaded = pickle.load(f)\n",
        "\n",
        "stacking_pipeline_loaded = joblib.load(\"stacking_pipeline.pkl\")\n",
        "\n",
        "with open(\"hybrid_weights.pkl\", \"rb\") as f:\n",
        "    weights_loaded = pickle.load(f)\n",
        "\n",
        "# Load historical data\n",
        "mhpi_df = pd.read_csv(\"https://raw.githubusercontent.com/englian1123/KL-High-Rise-Data/refs/heads/main/MHPI_Annual.csv\")\n",
        "last_year = int(mhpi_df['Year of Year'].iloc[-1])\n",
        "\n",
        "# Request schema\n",
        "class ForecastRequest(BaseModel):\n",
        "    desired_year: int\n",
        "    SchemeName: str\n",
        "    ParcelArea: float\n",
        "    Mukim: str\n",
        "    UnitLevel: str\n",
        "    Tenure: int\n",
        "\n",
        "@app.post(\"/forecast\")\n",
        "def forecast_price(request: ForecastRequest):\n",
        "    desired_year = request.desired_year\n",
        "    steps_ahead = desired_year - last_year\n",
        "\n",
        "    if steps_ahead <= 0:\n",
        "        if desired_year == last_year:\n",
        "            arima_pred = mhpi_df['Price'].iloc[-1]\n",
        "        else:\n",
        "            raise HTTPException(status_code=400, detail=f\"Cannot forecast for past year {desired_year}. Last historical year is {last_year}.\")\n",
        "    else:\n",
        "        forecast_values = arima_loaded.forecast(steps=steps_ahead)\n",
        "        arima_pred = forecast_values.iloc[-1] if hasattr(forecast_values, 'iloc') else forecast_values[-1]\n",
        "\n",
        "    raw_input = pd.DataFrame({\n",
        "        'SchemeName': [request.SchemeName],\n",
        "        'ParcelArea': [request.ParcelArea],\n",
        "        'Mukim': [request.Mukim],\n",
        "        'UnitLevel': [request.UnitLevel],\n",
        "        'Tenure': [request.Tenure]\n",
        "    })\n",
        "\n",
        "    stack_pred = np.expm1(stacking_pipeline_loaded.predict(raw_input))\n",
        "    hybrid_pred = (weights_loaded['weight_arima'] * arima_pred) + (weights_loaded['weight_stack'] * stack_pred[0])\n",
        "\n",
        "    return {\n",
        "        \"desired_year\": desired_year,\n",
        "        \"last_year\": last_year,\n",
        "        \"ARIMA_Prediction\": float(arima_pred),\n",
        "        \"Stacking_Prediction\": float(stack_pred[0]),\n",
        "        \"Hybrid_Forecast\": float(hybrid_pred)\n",
        "    }\n",
        "\n",
        "# Expose the app using ngrok\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Function to run uvicorn in a separate thread\n",
        "def run_uvicorn(app, port):\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
        "\n",
        "# Create and start the thread\n",
        "uvicorn_thread = threading.Thread(target=run_uvicorn, args=(app, 8000))\n",
        "uvicorn_thread.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9Xx0YWhPJkT",
        "outputId": "d555dc52-0f8d-4df6-e265-7b7573bf59c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.121.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.38.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: starlette<0.50.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.49.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.11.10)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.50.0,>=0.40.0->fastapi) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Public URL: NgrokTunnel: \"https://nonsectorial-cami-bankerly.ngrok-free.dev\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [1569]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "curl -X POST \"https://nonsectorial-cami-bankerly.ngrok-free.dev/forecast\" \\\n",
        "-H \"Content-Type: application/json\" \\\n",
        "-d '{\n",
        "      \"desired_year\": 2028,\n",
        "      \"SchemeName\": \"FERNLEA COURT\",\n",
        "      \"ParcelArea\": 1200,\n",
        "      \"Mukim\": \"Mukim Batu\",\n",
        "      \"UnitLevel\": \"12B\",\n",
        "      \"Tenure\": 1\n",
        "    }'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eDkOvxJVTFe",
        "outputId": "34e89440-0ff7-4f48-e57a-6fa138bcf589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html class=\"h-full\" lang=\"en-US\" dir=\"ltr\">\n",
            "  <head>\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-Regular-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-RegularItalic-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-Medium-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-Semibold-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/euclid-square/EuclidSquare-MediumItalic-WebS.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/ibm-plex-mono/IBMPlexMono-Text.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/ibm-plex-mono/IBMPlexMono-TextItalic.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/ibm-plex-mono/IBMPlexMono-SemiBold.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <link rel=\"preload\" href=\"https://cdn.ngrok.com/static/fonts/ibm-plex-mono/IBMPlexMono-SemiBoldItalic.woff\" as=\"font\" type=\"font/woff\" crossorigin=\"anonymous\" />\n",
            "    <meta charset=\"utf-8\">\n",
            "    <meta name=\"author\" content=\"ngrok\">\n",
            "    <meta name=\"description\" content=\"ngrok is the fastest way to put anything on the internet with a single command.\">\n",
            "    <meta name=\"robots\" content=\"noindex, nofollow\">\n",
            "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
            "    <link id=\"style\" rel=\"stylesheet\" href=\"https://cdn.ngrok.com/static/css/error.css\">\n",
            "    <noscript>The endpoint nonsectorial-cami-bankerly.ngrok-free.dev is offline. (ERR_NGROK_3200)</noscript>\n",
            "    <script id=\"script\" src=\"https://cdn.ngrok.com/static/js/error.js\" type=\"text/javascript\"></script>\n",
            "  </head>\n",
            "  <body class=\"h-full\" id=\"ngrok\">\n",
            "    <div id=\"root\" data-payload=\"eyJjZG5CYXNlIjoiaHR0cHM6Ly9jZG4ubmdyb2suY29tLyIsImNvZGUiOiIzMjAwIiwibWVzc2FnZSI6IlRoZSBlbmRwb2ludCBub25zZWN0b3JpYWwtY2FtaS1iYW5rZXJseS5uZ3Jvay1mcmVlLmRldiBpcyBvZmZsaW5lLiIsInRpdGxlIjoiTm90IEZvdW5kIn0=\"></div>\n",
            "  </body>\n",
            "</html>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2642  100  2471  100   171  28601   1979 --:--:-- --:--:-- --:--:-- 30720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()\n",
        "print(\"✅ ngrok tunnel stopped.\")"
      ],
      "metadata": {
        "id": "uI69gjHamOUQ",
        "outputId": "711290f7-b06c-4217-8785-c98a28f584f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ ngrok tunnel stopped.\n"
          ]
        }
      ]
    }
  ]
}